# src/sponsr.py
"""–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è Sponsr.ru"""

import json
import re

from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import html2text

from .config import Config, Source, load_cookie
from .database import Database
from .downloader import BaseDownloader, Post

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è embed URL –≤ watch URL
VIDEO_EMBED_PATTERNS = [
    (r'rutube\.ru/play/embed/([a-f0-9]+)', lambda m: f'https://rutube.ru/video/{m.group(1)}/'),
    (r'youtube\.com/embed/([^/?]+)', lambda m: f'https://youtube.com/watch?v={m.group(1)}'),
    (r'youtu\.be/([^/?]+)', lambda m: f'https://youtube.com/watch?v={m.group(1)}'),
    (r'player\.vimeo\.com/video/(\d+)', lambda m: f'https://vimeo.com/{m.group(1)}'),
    (r'ok\.ru/videoembed/(\d+)', lambda m: f'https://ok.ru/video/{m.group(1)}'),
    (r'vk\.com/video_ext\.php\?.*?oid=(-?\d+).*?id=(\d+)', lambda m: f'https://vk.com/video{m.group(1)}_{m.group(2)}'),
]


class SponsorDownloader(BaseDownloader):
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ —Å—Ç–∞—Ç–µ–π —Å Sponsr.ru"""

    PLATFORM = "sponsr"

    def __init__(self, config: Config, source: Source, db: Database):
        self._project_id: str | None = None
        super().__init__(config, source, db)

    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å cookies."""
        cookie = load_cookie(self.config.auth.sponsr_cookie_file)
        self.session.headers.update({
            'Cookie': cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'X-Requested-With': 'XMLHttpRequest',
        })

    def _get_project_id(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç project_id —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–∞."""
        if self._project_id:
            return self._project_id

        url = f"https://sponsr.ru/{self.source.author}/"
        response = self.session.get(url, timeout=self.TIMEOUT)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'lxml')
        data_tag = soup.find('script', id='__NEXT_DATA__')
        if not data_tag:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω __NEXT_DATA__ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")

        data = json.loads(data_tag.string)
        project_id = data.get('props', {}).get('pageProps', {}).get('project', {}).get('id')
        if not project_id:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω project.id –≤ __NEXT_DATA__")

        self._project_id = str(project_id)
        return self._project_id

    def fetch_posts_list(
        self,
        existing_ids: set[str] | None = None,
        incremental: bool = False,
        safety_chunks: int = 1
    ) -> list[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ API.
        
        Args:
            existing_ids: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö post_id (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞)
            incremental: –í–∫–ª—é—á–∏—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            safety_chunks: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ "–∑–∞—â–∏—Ç–Ω—ã—Ö" —á–∞–Ω–∫–æ–≤ –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
        """
        project_id = self._get_project_id()
        all_posts = []
        offset = 0
        clean_chunks_count = 0  # –°—á—ë—Ç—á–∏–∫ "—á–∏—Å—Ç—ã—Ö" —á–∞–Ω–∫–æ–≤

        while True:
            api_url = f"https://sponsr.ru/project/{project_id}/more-posts/?offset={offset}"
            response = self.session.get(api_url, timeout=self.TIMEOUT)
            response.raise_for_status()

            data = response.json().get("response", {})
            posts_chunk = data.get("rows", [])

            if not posts_chunk:
                break

            all_posts.extend(posts_chunk)
            offset = len(all_posts)

            total = data.get("rows_count", 0)

            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –ø–æ—Å—Ç—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            if incremental and existing_ids is not None:
                chunk_ids = {str(p.get('post_id')) for p in posts_chunk}
                all_existing = chunk_ids.issubset(existing_ids)

                if all_existing:
                    clean_chunks_count += 1
                    print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤... (—á–∞–Ω–∫ —É–∂–µ —Å–∫–∞—á–∞–Ω)")
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø–æ—Å–ª–µ safety_chunks + 1 (–ø–µ—Ä–≤—ã–π —á–∏—Å—Ç—ã–π + N –∑–∞—â–∏—Ç–Ω—ã—Ö)
                    if clean_chunks_count > safety_chunks:
                        print(f"  ‚ö° –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {offset} –ø–æ—Å—Ç–∞—Ö (–≤—Å–µ –Ω–æ–≤—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã)")
                        break
                else:
                    clean_chunks_count = 0
                    print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤...")
            else:
                print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤...")

        return all_posts

    def fetch_post(self, post_id: str) -> Post | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–¥–∏–Ω –ø–æ—Å—Ç –ø–æ ID."""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å—Ç–∞
        post = self._fetch_post_from_page(post_id)
        if post:
            return post

        # Fallback: –∏—â–µ–º –≤ API –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞)
        return self._find_post_in_api(post_id)

    def _fetch_post_from_page(self, post_id: str) -> Post | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å—Ç –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        # URL —Ñ–æ—Ä–º–∞—Ç: https://sponsr.ru/{author}/{post_id}/...
        url = f"https://sponsr.ru/{self.source.author}/{post_id}/"
        try:
            response = self.session.get(url, timeout=self.TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'lxml')
            data_tag = soup.find('script', id='__NEXT_DATA__')
            if not data_tag:
                return None

            data = json.loads(data_tag.string)
            post_data = data.get('props', {}).get('pageProps', {}).get('post')
            if not post_data:
                return None

            return self._parse_post(post_data)
        except requests.RequestException:
            return None

    def _find_post_in_api(self, post_id: str) -> Post | None:
        """–ò—â–µ—Ç –ø–æ—Å—Ç –≤ API –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ (–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–∏)."""
        project_id = self._get_project_id()
        offset = 0

        while True:
            api_url = f"https://sponsr.ru/project/{project_id}/more-posts/?offset={offset}"
            try:
                response = self.session.get(api_url, timeout=self.TIMEOUT)
                response.raise_for_status()

                data = response.json().get("response", {})
                posts_chunk = data.get("rows", [])

                if not posts_chunk:
                    break

                for raw_post in posts_chunk:
                    if str(raw_post.get('post_id')) == post_id:
                        return self._parse_post(raw_post)

                offset += len(posts_chunk)
            except requests.RequestException:
                break

        return None

    def _parse_post(self, raw_data: dict) -> Post:
        """–ü–∞—Ä—Å–∏—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ API –≤ Post."""
        post_id = str(raw_data.get('post_id') or raw_data.get('id'))
        title = raw_data.get('post_title') or raw_data.get('title') or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
        post_date = raw_data.get('post_date') or raw_data.get('date') or ''

        # URL –ø–æ—Å—Ç–∞
        post_url = raw_data.get('post_url') or f"/{self.source.author}/{post_id}/"
        if post_url and not post_url.startswith('http'):
            post_url = f"https://sponsr.ru{post_url}"

        # HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        content_obj = raw_data.get('post_text') or raw_data.get('text')
        if isinstance(content_obj, dict):
            content_html = content_obj.get('text', '')
        elif isinstance(content_obj, str):
            content_html = content_obj
        else:
            content_html = ''

        # –¢–µ–≥–∏ - –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞ –∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤
        tags_raw = raw_data.get('tags', [])
        tags = []
        if isinstance(tags_raw, list):
            for tag in tags_raw:
                if isinstance(tag, dict):
                    # API –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª–µ–º tag_name –∏–ª–∏ tag.tag_name
                    tag_name = tag.get('tag_name') or tag.get('tag', {}).get('tag_name')
                    if tag_name:
                        tags.append(tag_name)
                elif isinstance(tag, str):
                    tags.append(tag)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º assets –∏–∑ HTML
        assets = self._extract_assets(content_html)

        return Post(
            post_id=post_id,
            title=title,
            content_html=content_html,
            post_date=post_date,
            source_url=post_url,
            tags=tags,
            assets=assets,
        )

    def _extract_assets(self, html_content: str) -> list[dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ HTML."""
        if not html_content:
            return []

        assets = []
        soup = BeautifulSoup(html_content, 'lxml')

        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if not src:
                continue

            # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π URL
            if not src.startswith('http'):
                src = urljoin('https://sponsr.ru', src)

            # Alt —Ç–µ–∫—Å—Ç
            alt = img.get('alt', '')
            if not alt:
                parent = img.find_parent('div', class_='post-image')
                if parent and parent.get('data-alt'):
                    alt = parent.get('data-alt')

            assets.append({'url': src, 'alt': alt})

        return assets

    def _parse_video_url(self, embed_src: str) -> str | None:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç embed URL –≤ watch URL."""
        for pattern, converter in VIDEO_EMBED_PATTERNS:
            match = re.search(pattern, embed_src)
            if match:
                return converter(match)
        # Fallback: –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL –µ—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω
        if embed_src and ('video' in embed_src or 'embed' in embed_src):
            return embed_src
        return None

    def _replace_video_embeds(self, html_content: str) -> str:
        """–ó–∞–º–µ–Ω—è–µ—Ç iframe/embed –≤–∏–¥–µ–æ –Ω–∞ markdown-—Å—Å—ã–ª–∫–∏."""
        soup = BeautifulSoup(html_content, 'lxml')

        for iframe in soup.find_all(['iframe', 'embed']):
            src = iframe.get('src', '')
            video_url = self._parse_video_url(src)
            if video_url:
                placeholder = soup.new_tag('p')
                placeholder.string = f'üìπ –í–∏–¥–µ–æ: {video_url}'
                iframe.replace_with(placeholder)

        return str(soup)

    def _cleanup_html(self, html: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ HTML –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π –≤ Markdown."""
        from bs4.element import NavigableString, Tag

        soup = BeautifulSoup(html, 'lxml')
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É—Å—Ç—ã–µ)
        for tag in reversed(list(soup.find_all(['b', 'strong', 'em', 'i']))):
            # –í–∞–∂–Ω–æ: –Ω–µ —É–¥–∞–ª—è–µ–º —Ç–µ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–æ—Ä–∞—á–∏–≤–∞—é—Ç –¥—Ä—É–≥–∏–µ —Ç–µ–≥–∏,
            # –Ω–∞–ø—Ä–∏–º–µ—Ä <em><img/></em> –∏–ª–∏ <strong><br/></strong>.
            if tag.find(True) is not None:
                continue
            text = tag.get_text()
            if not text or text.isspace():
                tag.decompose()

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–∑–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω:
        #   <em>LEFT</em><a ...><em>MID</em></a><em>RIGHT</em>
        # –≤:
        #   <em>LEFT <a ...>MID</a> RIGHT</em>
        # (–ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Ç–µ–≥–∞–º–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è)
        def _is_ws_node(node: object) -> bool:
            return isinstance(node, NavigableString) and not str(node).strip()

        def _prev_non_ws_sibling(node: Tag) -> object | None:
            sib = node.previous_sibling
            while sib is not None and _is_ws_node(sib):
                sib = sib.previous_sibling
            return sib

        def _next_non_ws_sibling(node: Tag) -> object | None:
            sib = node.next_sibling
            while sib is not None and _is_ws_node(sib):
                sib = sib.next_sibling
            return sib

        def _starts_with_ws(text: str) -> bool:
            return bool(text) and text[0].isspace()

        def _needs_space_after(text: str) -> bool:
            if not text:
                return False
            last = text[-1]
            return last.isalnum() or last in ',;:'

        def _needs_space_before(text: str) -> bool:
            return bool(text) and text[0].isalnum()

        def _rstrip_ws_to_nbsp(tag: Tag) -> None:
            """–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç —Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–±–µ–ª—ã/—Ç–∞–±—ã –≤ NBSP.

            –í–∞–∂–Ω–æ: –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ (\n), —á—Ç–æ–±—ã –Ω–µ "—Å—Ö–ª–æ–ø—ã–≤–∞—Ç—å"
            –Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã.
            """
            if not tag.contents:
                return
            last = tag.contents[-1]
            if not isinstance(last, NavigableString):
                return
            s = str(last)
            m = re.search(r'[ \t]+$', s)
            if not m:
                return
            base = s[:m.start()]
            if base:
                last.replace_with(base)
            else:
                last.extract()
            # bs4 —Å–æ–∑–¥–∞—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —É–∑–µ–ª (NavigableString)
            tag.append('\xa0')

        def _lstrip_ws_to_nbsp(node: NavigableString) -> None:
            s = str(node)
            m = re.match(r'^[ \t]+', s)
            if not m:
                return
            node.replace_with('\xa0' + s[m.end():])

        for a in list(soup.find_all('a')):
            left = _prev_non_ws_sibling(a)
            right = _next_non_ws_sibling(a)
            if not (isinstance(left, Tag) and left.name == 'em'):
                continue
            if not (isinstance(right, Tag) and right.name == 'em'):
                continue

            # –£–∑–∫–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ: —Å–Ω–∞—Ä—É–∂–∏ –∏ –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏ –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤.
            if left.find(True) is not None or right.find(True) is not None:
                continue

            inner_tags = [c for c in a.contents if isinstance(c, Tag)]
            if len(inner_tags) != 1 or inner_tags[0].name != 'em':
                continue
            inner_em = inner_tags[0]
            if inner_em.find(True) is not None:
                continue
            if any(
                isinstance(c, NavigableString) and str(c).strip()
                for c in a.contents
                if not isinstance(c, Tag)
            ):
                continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ —É–∑–∫–∏–º –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º: –Ω–µ —Å–ª–∏–≤–∞–µ–º,
            # –µ—Å–ª–∏ –∞—Ç—Ä–∏–±—É—Ç—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è.
            left_attrs = dict(left.attrs or {})
            mid_attrs = dict(inner_em.attrs or {})
            right_attrs = dict(right.attrs or {})
            if not (not left_attrs and not mid_attrs and not right_attrs):
                if not (left_attrs == mid_attrs == right_attrs):
                    continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–µ–∂–¥—É em/a/em –Ω–µ—Ç –Ω–∏—á–µ–≥–æ –∫—Ä–æ–º–µ whitespace.
            between_left_a: list[NavigableString] = []
            node = left.next_sibling
            ok = True
            while node is not None and node is not a:
                if not _is_ws_node(node):
                    ok = False
                    break
                between_left_a.append(node)
                node = node.next_sibling
            if not ok or node is None:
                continue

            between_a_right: list[NavigableString] = []
            node = a.next_sibling
            while node is not None and node is not right:
                if not _is_ws_node(node):
                    ok = False
                    break
                between_a_right.append(node)
                node = node.next_sibling
            if not ok or node is None:
                continue

            left_text = left.get_text() or ''
            mid_text = inner_em.get_text() or ''
            right_text = right.get_text() or ''

            import copy

            new_em = soup.new_tag('em')
            new_em.attrs = copy.deepcopy(left.attrs)

            for child in list(left.contents):
                new_em.append(child.extract())
            for n in between_left_a:
                new_em.append(n.extract())

            # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª –±—ã–ª –≤ –∫–æ–Ω—Ü–µ LEFT –∏–ª–∏ –º–µ–∂–¥—É —Ç–µ–≥–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∫–∞–∫ NBSP,
            # —á—Ç–æ–±—ã html2text –Ω–µ "—Å—ä–µ–ª" –µ–≥–æ –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–æ–π.
            _rstrip_ws_to_nbsp(new_em)
            # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª–∞ –Ω–µ—Ç, –Ω–æ –æ–Ω –Ω—É–∂–µ–Ω –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏, –¥–æ–±–∞–≤–ª—è–µ–º NBSP.
            if (
                not between_left_a
                and not _starts_with_ws(mid_text)
                and _needs_space_after(left_text)
                and _needs_space_before(mid_text)
            ):
                new_em.append('\xa0')

            inner_em.unwrap()  # <a><em>..</em></a> -> <a>..</a>
            new_em.append(a.extract())

            for n in between_a_right:
                new_em.append(n.extract())

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –º–µ–∂–¥—É </a> –∏ RIGHT.
            _rstrip_ws_to_nbsp(new_em)

            # –ï—Å–ª–∏ RIGHT –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø—Ä–æ–±–µ–ª–∞/—Ç–∞–±–∞, –ø—Ä–µ–≤—Ä–∞—Ç–∏–º –µ–≥–æ –≤ NBSP.
            if right.contents and isinstance(right.contents[0], NavigableString):
                _lstrip_ws_to_nbsp(right.contents[0])

            # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª–∞ –Ω–µ—Ç, –Ω–æ –æ–Ω –Ω—É–∂–µ–Ω –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏, –¥–æ–±–∞–≤–ª—è–µ–º NBSP.
            if (
                not between_a_right
                and not _starts_with_ws(right_text)
                and (mid_text and mid_text[-1].isalnum())
                and (right_text and right_text[0].isalnum())
            ):
                new_em.append('\xa0')
            for child in list(right.contents):
                new_em.append(child.extract())

            left.replace_with(new_em)
            right.extract()
        
        return str(soup)

    def _to_markdown(self, post: Post, asset_map: dict[str, str]) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown."""
        if not post.content_html:
            return ""

        # –ó–∞–º–µ–Ω—è–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ
        html = post.content_html
        for original_url, local_filename in asset_map.items():
            html = html.replace(original_url, f"assets/{local_filename}")

        # –ó–∞–º–µ–Ω—è–µ–º iframe/embed –≤–∏–¥–µ–æ –Ω–∞ markdown-—Å—Å—ã–ª–∫–∏
        html = self._replace_video_embeds(html)
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ HTML
        html = self._cleanup_html(html)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ Markdown
        h2t = html2text.HTML2Text()
        h2t.ignore_links = False
        h2t.ignore_images = False
        h2t.body_width = 0  # –ë–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫
        h2t.unicode_snob = True

        markdown = h2t.handle(html)

        # –£–¥–∞–ª—è–µ–º bidi-–º–∞—Ä–∫–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–º–∞—é—Ç –ø—Ä–æ–±–µ–ª—ã —Ä—è–¥–æ–º —Å —Ç–µ–∫—Å—Ç–æ–º
        markdown = re.sub(r'[\u200e\u200f\u202a-\u202e\u2066-\u2069]', '', markdown)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        markdown = re.sub(r'[\u00a0\u202f]', ' ', markdown)

        # –°–∫–ª–µ–∏–≤–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ em/strong –≤ –∂–∏—Ä–Ω—ã–π –∫—É—Ä—Å–∏–≤
        # html2text —Å–æ–∑–¥–∞—ë—Ç ** _—Ç–µ–∫—Å—Ç_** –∏–ª–∏ _**—Ç–µ–∫—Å—Ç**_ –¥–ª—è <b><em> (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏)
        markdown = re.sub(r'\*\*\s*_(.+?)_\s*\*\*', r'***\1***', markdown)
        markdown = re.sub(r'_\s*\*\*(.+?)\*\*\s*_', r'***\1***', markdown)
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä—å —Å—Å—ã–ª–æ–∫
        # [** _—Ç–µ–∫—Å—Ç_**](url) ‚Üí [***—Ç–µ–∫—Å—Ç***](url)
        markdown = re.sub(r'\[(\*{2,3})\s*(.+?)\s*(\*{2,3})\]\((.+?)\)', r'[\1\2\3](\4)', markdown)
        # ***[—Ç–µ–∫—Å—Ç](url)*** ‚Üí [***—Ç–µ–∫—Å—Ç***](url)
        markdown = re.sub(r'(\*{2,3})\[(.+?)\]\((.+?)\)\1', r'[\1\2\1](\3)', markdown)
        # _[—Ç–µ–∫—Å—Ç](url)_ ‚Üí [_—Ç–µ–∫—Å—Ç_](url)
        markdown = re.sub(r'_\[(.+?)\]\((.+?)\)_', r'[_\1_](\2)', markdown)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ html2text —Ä—è–¥–æ–º —Å Unicode-–∫–∞–≤—ã—á–∫–∞–º–∏
        # –û—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ: ¬´ ‚Äû " '
        markdown = re.sub(r'([\u00ab\u201e\u201c\u2018])\s+', r'\1', markdown)
        # –ó–∞–∫—Ä—ã–≤–∞—é—â–∏–µ: ¬ª " '
        markdown = re.sub(r'\s+([\u00bb\u201d\u2019])', r'\1', markdown)

        # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –≤–æ–∫—Ä—É–≥ Markdown-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.
        # –ü—Ä–∏–Ω—Ü–∏–ø: –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –ø—Ä–æ–±–µ–ª—ã "–≤—Å–ª–µ–ø—É—é" (–∏ —Ç–µ–º –±–æ–ª–µ–µ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤),
        # –∞ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å —Ç–æ–ª—å–∫–æ —É–∑–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã html2text/–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
        def _cleanup_spacing(text: str) -> str:
            # 1) –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–æ–∫ —Å—Å—ã–ª–∫–∏, –∫–æ–≥–¥–∞ —Ç–∞–º –µ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
            #    [ _–ö–æ–Ω–∞–Ω_ ] -> [_–ö–æ–Ω–∞–Ω_]
            text = re.sub(r'\[[ \t]+([_*])', r'[\1', text)
            text = re.sub(r'([_*])[ \t]+\]', r'\1]', text)

            # 1.5) –¢—Ä–∏–º–∏–º –ø—Ä–æ–±–µ–ª—ã/—Ç–∞–±—ã —Å—Ä–∞–∑—É –≤–Ω—É—Ç—Ä–∏ –º–∞—Ä–∫–µ—Ä–æ–≤ emphasis.
            # html2text –∏–Ω–æ–≥–¥–∞ —Å–æ–∑–¥–∞—ë—Ç `_ —Ç–µ–∫—Å—Ç _` (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –ø—Ä–æ–±–µ–ª—ã –∏–¥—É—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —É–∑–ª–∞–º–∏ span/NBSP).
            def _trim_em_inner(delim: str, inner: str) -> str:
                trimmed = inner.strip(' \t')
                return f"{delim}{trimmed}{delim}" if trimmed else f"{delim}{inner}{delim}"

            text = re.sub(r'\*\*\*([^*\n]+?)\*\*\*', lambda m: _trim_em_inner('***', m.group(1)), text)
            text = re.sub(r'(?<!\*)\*\*([^*\n]+?)\*\*(?!\*)', lambda m: _trim_em_inner('**', m.group(1)), text)
            text = re.sub(r'(?<!\*)\*([^*\n]+?)\*(?!\*)', lambda m: _trim_em_inner('*', m.group(1)), text)
            text = re.sub(r'_([^_\n]+?)_', lambda m: _trim_em_inner('_', m.group(1)), text)

            # 2) –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
            #    ***...*** : -> ***...***:
            emphasis_span = r'(?:\*\*\*[^*\n]+?\*\*\*|\*\*[^*\n]+?\*\*|_[^_\n]+?_|\*[^*\n]+?\*)'
            text = re.sub(rf'({emphasis_span})[ \t]+([:;,.!?])', r'\1\2', text)

            word_char = r'[0-9A-Za-z–ê-–Ø–∞-—è–Å—ë]'

            # 3) html2text –∏–Ω–æ–≥–¥–∞ "—Å—ä–µ–¥–∞–µ—Ç" –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ **–∂–∏—Ä–Ω—ã–º** –≤–Ω—É—Ç—Ä–∏ _–∫—É—Ä—Å–∏–≤–∞_.
            #    _–∫—É—Ä—Å–∏–≤**–∂–∏—Ä–Ω—ã–π** –∫—É—Ä—Å–∏–≤_ -> _–∫—É—Ä—Å–∏–≤ **–∂–∏—Ä–Ω—ã–π** –∫—É—Ä—Å–∏–≤_
            def _fix_bold_spacing_inside_underscore_italic(m: re.Match) -> str:
                inner = m.group('inner')
                inner = re.sub(
                    rf'(?P<l>{word_char})\*\*(?P<b>[^*\n]+?)\*\*(?=[ \t]+{word_char})',
                    r'\g<l> **\g<b>**',
                    inner,
                )
                return f"_{inner}_"

            text = re.sub(r'_(?P<inner>[^_\n]+?)_', _fix_bold_spacing_inside_underscore_italic, text)

            # 4) –°–∫–ª–µ–∏–≤–∞–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, –∫–æ–≥–¥–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞.
            #    –ø ***–æ*** —Ç–æ–º -> –ø***–æ***—Ç–æ–º
            # –í–∞—Ä–∏–∞–Ω—Ç—ã –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: –ø—Ä–æ–±–µ–ª—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Å –æ–±–µ–∏—Ö —Å—Ç–æ—Ä–æ–Ω –∏–ª–∏ —Ç–æ–ª—å–∫–æ —Å –æ–¥–Ω–æ–π.
            inside_word_both = re.compile(rf'(?P<l>{word_char})[ \t]+(?P<em>{emphasis_span})[ \t]+(?P<r>{word_char})')
            inside_word_left = re.compile(rf'(?P<l>{word_char})[ \t]+(?P<em>{emphasis_span})(?P<r>{word_char})')
            inside_word_right = re.compile(rf'(?P<l>{word_char})(?P<em>{emphasis_span})[ \t]+(?P<r>{word_char})')
            common_one_letter_words = {
                # ru
                '–∏', '–∞', '—è', '–æ', '—É', '–≤', '–∫', '—Å',
                # en
                'a', 'i',
            }

            def _em_inner(em: str) -> str:
                for pre, suf in (("***", "***"), ("**", "**"), ("_", "_"), ("*", "*")):
                    if em.startswith(pre) and em.endswith(suf) and len(em) >= len(pre) + len(suf):
                        return em[len(pre) : -len(suf)]
                return em

            def _is_short_emphasis(em: str) -> bool:
                inner = _em_inner(em).strip()
                if re.search(r'\s', inner):
                    return False
                return re.fullmatch(rf'{word_char}{{1,3}}', inner) is not None

            def _join_if_inside_word(m: re.Match, *, require_short: bool) -> str:
                l = m.group('l')
                r = m.group('r')

                if require_short and not _is_short_emphasis(m.group('em')):
                    return m.group(0)

                # –ï—Å–ª–∏ —Å–ª–µ–≤–∞ –æ–¥–Ω–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ ("–∏", "–∞", "–≤"...),
                # –ª—É—á—à–µ –Ω–µ —Å–∫–ª–µ–∏–≤–∞—Ç—å: –≤—ã—Å–æ–∫ —Ä–∏—Å–∫ "–ø–æ—á–∏–Ω–∏—Ç—å" –∞–≤—Ç–æ—Ä—Å–∫–∏–π —Ç–µ–∫—Å—Ç.
                i = m.start('l')
                prev = text[i - 1] if i > 0 else ''
                if (i == 0 or prev.isspace()) and l.lower() in common_one_letter_words:
                    return m.group(0)

                return f"{l}{m.group('em')}{r}"

            text = inside_word_both.sub(lambda m: _join_if_inside_word(m, require_short=True), text)
            text = inside_word_left.sub(lambda m: _join_if_inside_word(m, require_short=False), text)
            # –ï—Å–ª–∏ —Å–ª–µ–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∂–µ –ø—Ä–∏–∫–ª–µ–µ–Ω–æ –∫ —Å–ª–æ–≤—É, –∞ –ø—Ä–æ–±–µ–ª –æ—Å—Ç–∞–ª—Å—è —Å–ø—Ä–∞–≤–∞,
            # —ç—Ç–æ –ø–æ—á—Ç–∏ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ —Ä–∞–∑—Ä—ã–≤ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞.
            text = inside_word_right.sub(lambda m: _join_if_inside_word(m, require_short=False), text)

            # 5) –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–æ–π.
            #    –∫–∏–Ω–æ–ø–æ–ª–æ—Ç–Ω–∞,[–ö–æ–Ω–∞–Ω](...) -> –∫–∏–Ω–æ–ø–æ–ª–æ—Ç–Ω–∞, [–ö–æ–Ω–∞–Ω](...)
            text = re.sub(r',[ \t]*(\[[^\]]+\]\([^)]+\))', r', \1', text)

            # 6) –†–∞–∑–¥–µ–ª—è–µ–º —Å–ª–æ–≤–∞ –∏ markdown-—Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ "—Å–ª–∏–ø–ª–∏—Å—å".
            #    –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ªa[...](...)–∏ -> –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ªa [...](...) –∏
            link = r'(?:\[[^\]]+\]\([^)]+\))'
            text = re.sub(rf'({word_char})({link})', r'\1 \2', text)
            text = re.sub(rf'({link})({word_char})', r'\1 \2', text)

            return text

        markdown = _cleanup_spacing(markdown)

        # Markdown (CommonMark/Goldmark): `_em_` –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –ù–ï —Ä–µ–Ω–¥–µ—Ä–∏—Ç—Å—è –∫–∞–∫ –∫—É—Ä—Å–∏–≤.
        # –ï—Å–ª–∏ –∫—É—Ä—Å–∏–≤ "–≤—à–∏—Ç" –≤ —Å–ª–æ–≤–æ (–±—É–∫–≤–∞ + _..._ + –±—É–∫–≤–∞), –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –≤ `*...*`.
        # –ü—Ä–∞–≤–∏–ª–æ (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ):
        # - –≤—Å–µ–≥–¥–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º, –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å –ø—Ä–æ–±–µ–ª—ã/markdown-–º–∞—Ä–∫–µ—Ä—ã (—Ç–∏–ø–∏—á–Ω—ã–π –≤—ã–≤–æ–¥ html2text);
        # - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–¥–Ω–æ-—Ç—Ä—ë—Ö–±—É–∫–≤–µ–Ω–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π
        #   (–Ω–∞–ø—Ä–∏–º–µ—Ä: –ø_–æ_—Ç–æ–º -> –ø*–æ*—Ç–æ–º), —á—Ç–æ–±—ã Goldmark –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª –∫—É—Ä—Å–∏–≤.
        word_char = r'[0-9A-Za-z–ê-–Ø–∞-—è–Å—ë]'
        intraword_underscore_italic = re.compile(
            rf'(?P<l>{word_char})_(?P<inner>[^_\n]+?)_(?P<r>{word_char})'
        )

        def _intraword_underscore_to_asterisk(m: re.Match) -> str:
            l = m.group('l')
            inner = m.group('inner')
            r = m.group('r')
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π –Ω–∞ –ª–∏—Ç–µ—Ä–∞–ª–∞—Ö —Å –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è–º–∏ (foo_bar_baz).
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —ç—Ç–æ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ –∫—É—Ä—Å–∏–≤ –∏–∑ html2text:
            # –≤–Ω—É—Ç—Ä–∏ –æ–±—ã—á–Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–µ–ª—ã –∏/–∏–ª–∏ markdown-–º–∞—Ä–∫–µ—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä '**' –∏–ª–∏ '[...]').
            looks_like_html2text_em = any(ch.isspace() for ch in inner) or '*' in inner or '[' in inner

            # –°–ø–µ—Ü-—Å–ª—É—á–∞–π: –æ–¥–Ω–∞-—Ç—Ä–∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã –≤–Ω—É—Ç—Ä–∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
            # –≠—Ç–æ –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ, —á–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –≤—Å—Ç–∞–≤–∫–∏, –∏ –Ω–µ –ª–æ–º–∞–µ—Ç foo_bar_baz.
            cyr = r'[–ê-–Ø–∞-—è–Å—ë]'
            has_cyr_context = re.search(cyr, f"{l}{inner}{r}") is not None
            is_short_cyr_inner = re.fullmatch(rf'{cyr}{{1,3}}', inner) is not None

            if not (looks_like_html2text_em or (has_cyr_context and is_short_cyr_inner)):
                return m.group(0)
            return f"{l}*{inner}*{r}"

        markdown = intraword_underscore_italic.sub(_intraword_underscore_to_asterisk, markdown)

        return markdown
