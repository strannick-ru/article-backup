# src/sponsr.py
"""–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è Sponsr.ru"""

import json
import re

from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import html2text

from .config import Config, Source, load_cookie
from .database import Database
from .downloader import BaseDownloader, Post

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è embed URL –≤ watch URL
VIDEO_EMBED_PATTERNS = [
    (r'rutube\.ru/play/embed/([a-f0-9]+)', lambda m: f'https://rutube.ru/video/{m.group(1)}/'),
    (r'youtube\.com/embed/([^/?]+)', lambda m: f'https://youtube.com/watch?v={m.group(1)}'),
    (r'youtu\.be/([^/?]+)', lambda m: f'https://youtube.com/watch?v={m.group(1)}'),
    (r'player\.vimeo\.com/video/(\d+)', lambda m: f'https://vimeo.com/{m.group(1)}'),
    (r'ok\.ru/videoembed/(\d+)', lambda m: f'https://ok.ru/video/{m.group(1)}'),
    (r'vk\.com/video_ext\.php\?.*?oid=(-?\d+).*?id=(\d+)', lambda m: f'https://vk.com/video{m.group(1)}_{m.group(2)}'),
]


class SponsorDownloader(BaseDownloader):
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ —Å—Ç–∞—Ç–µ–π —Å Sponsr.ru"""

    PLATFORM = "sponsr"

    def __init__(self, config: Config, source: Source, db: Database):
        self._project_id: str | None = None
        super().__init__(config, source, db)

    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å cookies."""
        cookie = load_cookie(self.config.auth.sponsr_cookie_file)
        self.session.headers.update({
            'Cookie': cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'X-Requested-With': 'XMLHttpRequest',
        })

    def _get_project_id(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç project_id —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–∞."""
        if self._project_id:
            return self._project_id

        url = f"https://sponsr.ru/{self.source.author}/"
        response = self.session.get(url, timeout=self.TIMEOUT)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'lxml')
        data_tag = soup.find('script', id='__NEXT_DATA__')
        if not data_tag:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω __NEXT_DATA__ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")

        data = json.loads(data_tag.string)
        project_id = data.get('props', {}).get('pageProps', {}).get('project', {}).get('id')
        if not project_id:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω project.id –≤ __NEXT_DATA__")

        self._project_id = str(project_id)
        return self._project_id

    def fetch_posts_list(
        self,
        existing_ids: set[str] | None = None,
        incremental: bool = False,
        safety_chunks: int = 1
    ) -> list[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ API.
        
        Args:
            existing_ids: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö post_id (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞)
            incremental: –í–∫–ª—é—á–∏—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            safety_chunks: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ "–∑–∞—â–∏—Ç–Ω—ã—Ö" —á–∞–Ω–∫–æ–≤ –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
        """
        project_id = self._get_project_id()
        all_posts = []
        offset = 0
        clean_chunks_count = 0  # –°—á—ë—Ç—á–∏–∫ "—á–∏—Å—Ç—ã—Ö" —á–∞–Ω–∫–æ–≤

        while True:
            api_url = f"https://sponsr.ru/project/{project_id}/more-posts/?offset={offset}"
            response = self.session.get(api_url, timeout=self.TIMEOUT)
            response.raise_for_status()

            data = response.json().get("response", {})
            posts_chunk = data.get("rows", [])

            if not posts_chunk:
                break

            all_posts.extend(posts_chunk)
            offset = len(all_posts)

            total = data.get("rows_count", 0)

            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –ø–æ—Å—Ç—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            if incremental and existing_ids is not None:
                chunk_ids = {str(p.get('post_id')) for p in posts_chunk}
                all_existing = chunk_ids.issubset(existing_ids)

                if all_existing:
                    clean_chunks_count += 1
                    print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤... (—á–∞–Ω–∫ —É–∂–µ —Å–∫–∞—á–∞–Ω)")
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø–æ—Å–ª–µ safety_chunks + 1 (–ø–µ—Ä–≤—ã–π —á–∏—Å—Ç—ã–π + N –∑–∞—â–∏—Ç–Ω—ã—Ö)
                    if clean_chunks_count > safety_chunks:
                        print(f"  ‚ö° –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {offset} –ø–æ—Å—Ç–∞—Ö (–≤—Å–µ –Ω–æ–≤—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã)")
                        break
                else:
                    clean_chunks_count = 0
                    print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤...")
            else:
                print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤...")

        return all_posts

    def fetch_post(self, post_id: str) -> Post | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–¥–∏–Ω –ø–æ—Å—Ç –ø–æ ID."""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å—Ç–∞
        post = self._fetch_post_from_page(post_id)
        if post:
            return post

        # Fallback: –∏—â–µ–º –≤ API –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞)
        return self._find_post_in_api(post_id)

    def _fetch_post_from_page(self, post_id: str) -> Post | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å—Ç –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        # URL —Ñ–æ—Ä–º–∞—Ç: https://sponsr.ru/{author}/{post_id}/...
        url = f"https://sponsr.ru/{self.source.author}/{post_id}/"
        try:
            response = self.session.get(url, timeout=self.TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'lxml')
            data_tag = soup.find('script', id='__NEXT_DATA__')
            if not data_tag:
                return None

            data = json.loads(data_tag.string)
            post_data = data.get('props', {}).get('pageProps', {}).get('post')
            if not post_data:
                return None

            return self._parse_post(post_data)
        except requests.RequestException:
            return None

    def _find_post_in_api(self, post_id: str) -> Post | None:
        """–ò—â–µ—Ç –ø–æ—Å—Ç –≤ API –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ (–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–∏)."""
        project_id = self._get_project_id()
        offset = 0

        while True:
            api_url = f"https://sponsr.ru/project/{project_id}/more-posts/?offset={offset}"
            try:
                response = self.session.get(api_url, timeout=self.TIMEOUT)
                response.raise_for_status()

                data = response.json().get("response", {})
                posts_chunk = data.get("rows", [])

                if not posts_chunk:
                    break

                for raw_post in posts_chunk:
                    if str(raw_post.get('post_id')) == post_id:
                        return self._parse_post(raw_post)

                offset += len(posts_chunk)
            except requests.RequestException:
                break

        return None

    def _parse_post(self, raw_data: dict) -> Post:
        """–ü–∞—Ä—Å–∏—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ API –≤ Post."""
        post_id = str(raw_data.get('post_id') or raw_data.get('id'))
        title = raw_data.get('post_title') or raw_data.get('title') or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
        post_date = raw_data.get('post_date') or raw_data.get('date') or ''

        # URL –ø–æ—Å—Ç–∞
        post_url = raw_data.get('post_url') or f"/{self.source.author}/{post_id}/"
        if post_url and not post_url.startswith('http'):
            post_url = f"https://sponsr.ru{post_url}"

        # HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        content_obj = raw_data.get('post_text') or raw_data.get('text')
        if isinstance(content_obj, dict):
            content_html = content_obj.get('text', '')
        elif isinstance(content_obj, str):
            content_html = content_obj
        else:
            content_html = ''

        # –¢–µ–≥–∏ - –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞ –∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤
        tags_raw = raw_data.get('tags', [])
        tags = []
        if isinstance(tags_raw, list):
            for tag in tags_raw:
                if isinstance(tag, dict):
                    # API –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª–µ–º tag_name –∏–ª–∏ tag.tag_name
                    tag_name = tag.get('tag_name') or tag.get('tag', {}).get('tag_name')
                    if tag_name:
                        tags.append(tag_name)
                elif isinstance(tag, str):
                    tags.append(tag)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º assets –∏–∑ HTML
        assets = self._extract_assets(content_html)

        return Post(
            post_id=post_id,
            title=title,
            content_html=content_html,
            post_date=post_date,
            source_url=post_url,
            tags=tags,
            assets=assets,
        )

    def _extract_assets(self, html_content: str) -> list[dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ HTML."""
        if not html_content:
            return []

        assets = []
        soup = BeautifulSoup(html_content, 'lxml')

        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if not src:
                continue

            # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π URL
            if not src.startswith('http'):
                src = urljoin('https://sponsr.ru', src)

            # Alt —Ç–µ–∫—Å—Ç
            alt = img.get('alt', '')
            if not alt:
                parent = img.find_parent('div', class_='post-image')
                if parent and parent.get('data-alt'):
                    alt = parent.get('data-alt')

            assets.append({'url': src, 'alt': alt})

        return assets

    def _parse_video_url(self, embed_src: str) -> str | None:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç embed URL –≤ watch URL."""
        for pattern, converter in VIDEO_EMBED_PATTERNS:
            match = re.search(pattern, embed_src)
            if match:
                return converter(match)
        # Fallback: –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL –µ—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω
        if embed_src and ('video' in embed_src or 'embed' in embed_src):
            return embed_src
        return None

    def _replace_video_embeds(self, html_content: str) -> str:
        """–ó–∞–º–µ–Ω—è–µ—Ç iframe/embed –≤–∏–¥–µ–æ –Ω–∞ markdown-—Å—Å—ã–ª–∫–∏."""
        soup = BeautifulSoup(html_content, 'lxml')

        for iframe in soup.find_all(['iframe', 'embed']):
            src = iframe.get('src', '')
            video_url = self._parse_video_url(src)
            if video_url:
                placeholder = soup.new_tag('p')
                placeholder.string = f'üìπ –í–∏–¥–µ–æ: {video_url}'
                iframe.replace_with(placeholder)

        return str(soup)

    def _cleanup_html(self, html: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ HTML –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π –≤ Markdown."""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html, 'lxml')
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É—Å—Ç—ã–µ)
        for tag in soup.find_all(['b', 'strong', 'em', 'i']):
            text = tag.get_text()
            if not text:
                tag.decompose()
            elif text.isspace():
                tag.replace_with(text)
        
        return str(soup)

    def _to_markdown(self, post: Post, asset_map: dict[str, str]) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown."""
        if not post.content_html:
            return ""

        # –ó–∞–º–µ–Ω—è–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ
        html = post.content_html
        for original_url, local_filename in asset_map.items():
            html = html.replace(original_url, f"assets/{local_filename}")

        # –ó–∞–º–µ–Ω—è–µ–º iframe/embed –≤–∏–¥–µ–æ –Ω–∞ markdown-—Å—Å—ã–ª–∫–∏
        html = self._replace_video_embeds(html)
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ HTML
        html = self._cleanup_html(html)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ Markdown
        h2t = html2text.HTML2Text()
        h2t.ignore_links = False
        h2t.ignore_images = False
        h2t.body_width = 0  # –ë–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫
        h2t.unicode_snob = True

        markdown = h2t.handle(html)

        # –£–¥–∞–ª—è–µ–º bidi-–º–∞—Ä–∫–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–º–∞—é—Ç –ø—Ä–æ–±–µ–ª—ã —Ä—è–¥–æ–º —Å —Ç–µ–∫—Å—Ç–æ–º
        markdown = re.sub(r'[\u200e\u200f\u202a-\u202e\u2066-\u2069]', '', markdown)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        markdown = re.sub(r'[\u00a0\u202f]', ' ', markdown)

        # –°–∫–ª–µ–∏–≤–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ em/strong –≤ –∂–∏—Ä–Ω—ã–π –∫—É—Ä—Å–∏–≤
        # html2text —Å–æ–∑–¥–∞—ë—Ç ** _—Ç–µ–∫—Å—Ç_** –∏–ª–∏ _**—Ç–µ–∫—Å—Ç**_ –¥–ª—è <b><em> (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏)
        markdown = re.sub(r'\*\*\s*_(.+?)_\s*\*\*', r'***\1***', markdown)
        markdown = re.sub(r'_\s*\*\*(.+?)\*\*\s*_', r'***\1***', markdown)
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä—å —Å—Å—ã–ª–æ–∫
        # [** _—Ç–µ–∫—Å—Ç_**](url) ‚Üí [***—Ç–µ–∫—Å—Ç***](url)
        markdown = re.sub(r'\[(\*{2,3})\s*(.+?)\s*(\*{2,3})\]\((.+?)\)', r'[\1\2\3](\4)', markdown)
        # ***[—Ç–µ–∫—Å—Ç](url)*** ‚Üí [***—Ç–µ–∫—Å—Ç***](url)
        markdown = re.sub(r'(\*{2,3})\[(.+?)\]\((.+?)\)\1', r'[\1\2\1](\3)', markdown)
        # _[—Ç–µ–∫—Å—Ç](url)_ ‚Üí [_—Ç–µ–∫—Å—Ç_](url)
        markdown = re.sub(r'_\[(.+?)\]\((.+?)\)_', r'[_\1_](\2)', markdown)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ html2text —Ä—è–¥–æ–º —Å Unicode-–∫–∞–≤—ã—á–∫–∞–º–∏
        # –û—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ: ¬´ ‚Äû " '
        markdown = re.sub(r'([\u00ab\u201e\u201c\u2018])\s+', r'\1', markdown)
        # –ó–∞–∫—Ä—ã–≤–∞—é—â–∏–µ: ¬ª " '
        markdown = re.sub(r'\s+([\u00bb\u201d\u2019])', r'\1', markdown)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Å—ã–ª–æ–∫
        def _fix_spacing(text: str, pattern: re.Pattern) -> str:
            """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
            parts = []
            last = 0
            for match in pattern.finditer(text):
                start, end = match.span()
                before = text[last:start]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª —Å–ª–µ–≤–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if start > 0 and before and before[-1].isalnum():
                    before = before + ' '
                
                parts.append(before)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º –º–∞—Ç—á
                matched_text = text[start:end]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª —Å–ø—Ä–∞–≤–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if end < len(text) and text[end].isalnum():
                    matched_text = matched_text + ' '
                
                parts.append(matched_text)
                last = end

            parts.append(text[last:])
            return ''.join(parts)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ bold-italic, bold, —Å—Å—ã–ª–æ–∫
        markdown = _fix_spacing(markdown, re.compile(r'\*\*\*.+?\*\*\*'))
        markdown = _fix_spacing(markdown, re.compile(r'(?<!\*)\*\*(?!\*).+?(?<!\*)\*\*(?!\*)'))
        markdown = _fix_spacing(markdown, re.compile(r'\[[^\]]+\]\([^)]+\)'))

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–µ—Ä—ë—Ç—Å—è –∏–∑ frontmatter (Hugo), –Ω–µ –¥—É–±–ª–∏—Ä—É–µ–º –µ–≥–æ –≤ body.
        return markdown
