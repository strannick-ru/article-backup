# src/sponsr.py
"""–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è Sponsr.ru"""

import json
import re
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import html2text

from .config import Config, Source, load_cookie
from .database import Database
from .downloader import BaseDownloader, Post

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è embed URL –≤ watch URL
VIDEO_EMBED_PATTERNS = [
    (r'rutube\.ru/play/embed/([a-f0-9]+)', lambda m: f'https://rutube.ru/video/{m.group(1)}/'),
    (r'youtube\.com/embed/([^/?]+)', lambda m: f'https://youtube.com/watch?v={m.group(1)}'),
    (r'youtu\.be/([^/?]+)', lambda m: f'https://youtube.com/watch?v={m.group(1)}'),
    (r'player\.vimeo\.com/video/(\d+)', lambda m: f'https://vimeo.com/{m.group(1)}'),
    (r'ok\.ru/videoembed/(\d+)', lambda m: f'https://ok.ru/video/{m.group(1)}'),
    (r'vk\.com/video_ext\.php\?.*?oid=(-?\d+).*?id=(\d+)', lambda m: f'https://vk.com/video{m.group(1)}_{m.group(2)}'),
]


class SponsorDownloader(BaseDownloader):
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ —Å—Ç–∞—Ç–µ–π —Å Sponsr.ru"""

    PLATFORM = "sponsr"

    def __init__(self, config: Config, source: Source, db: Database):
        self._project_id: str | None = None
        super().__init__(config, source, db)

    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å cookies."""
        cookie = load_cookie(self.config.auth.sponsr_cookie_file)
        self.session.headers.update({
            'Cookie': cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'X-Requested-With': 'XMLHttpRequest',
        })

    def _get_project_id(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç project_id —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–∞."""
        if self._project_id:
            return self._project_id

        url = f"https://sponsr.ru/{self.source.author}/"
        response = self.session.get(url, timeout=self.TIMEOUT)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'lxml')
        data_tag = soup.find('script', id='__NEXT_DATA__')
        if not data_tag:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω __NEXT_DATA__ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")

        data = json.loads(data_tag.string)
        project_id = data.get('props', {}).get('pageProps', {}).get('project', {}).get('id')
        if not project_id:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω project.id –≤ __NEXT_DATA__")

        self._project_id = str(project_id)
        return self._project_id

    def fetch_posts_list(self) -> list[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ API."""
        project_id = self._get_project_id()
        all_posts = []
        offset = 0

        while True:
            api_url = f"https://sponsr.ru/project/{project_id}/more-posts/?offset={offset}"
            response = self.session.get(api_url, timeout=self.TIMEOUT)
            response.raise_for_status()

            data = response.json().get("response", {})
            posts_chunk = data.get("rows", [])

            if not posts_chunk:
                break

            all_posts.extend(posts_chunk)
            offset = len(all_posts)

            total = data.get("rows_count", 0)
            print(f"  –ü–æ–ª—É—á–µ–Ω–æ {offset}/{total} –ø–æ—Å—Ç–æ–≤...")

        return all_posts

    def fetch_post(self, post_id: str) -> Post | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–¥–∏–Ω –ø–æ—Å—Ç –ø–æ ID."""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å—Ç–∞
        post = self._fetch_post_from_page(post_id)
        if post:
            return post

        # Fallback: –∏—â–µ–º –≤ API –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞)
        return self._find_post_in_api(post_id)

    def _fetch_post_from_page(self, post_id: str) -> Post | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å—Ç –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        # URL —Ñ–æ—Ä–º–∞—Ç: https://sponsr.ru/{author}/{post_id}/...
        url = f"https://sponsr.ru/{self.source.author}/{post_id}/"
        try:
            response = self.session.get(url, timeout=self.TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'lxml')
            data_tag = soup.find('script', id='__NEXT_DATA__')
            if not data_tag:
                return None

            data = json.loads(data_tag.string)
            post_data = data.get('props', {}).get('pageProps', {}).get('post')
            if not post_data:
                return None

            return self._parse_post(post_data)
        except requests.RequestException:
            return None

    def _find_post_in_api(self, post_id: str) -> Post | None:
        """–ò—â–µ—Ç –ø–æ—Å—Ç –≤ API –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ (–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–∏)."""
        project_id = self._get_project_id()
        offset = 0

        while True:
            api_url = f"https://sponsr.ru/project/{project_id}/more-posts/?offset={offset}"
            try:
                response = self.session.get(api_url, timeout=self.TIMEOUT)
                response.raise_for_status()

                data = response.json().get("response", {})
                posts_chunk = data.get("rows", [])

                if not posts_chunk:
                    break

                for raw_post in posts_chunk:
                    if str(raw_post.get('post_id')) == post_id:
                        return self._parse_post(raw_post)

                offset += len(posts_chunk)
            except requests.RequestException:
                break

        return None

    def _parse_post(self, raw_data: dict) -> Post:
        """–ü–∞—Ä—Å–∏—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ API –≤ Post."""
        post_id = str(raw_data['post_id'])
        title = raw_data.get('post_title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        post_date = raw_data.get('post_date', '')

        # URL –ø–æ—Å—Ç–∞
        post_url = raw_data.get('post_url', '')
        if post_url and not post_url.startswith('http'):
            post_url = f"https://sponsr.ru{post_url}"

        # HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        content_html = raw_data.get('post_text', '')

        # –¢–µ–≥–∏
        tags = raw_data.get('tags', [])

        # –ò–∑–≤–ª–µ–∫–∞–µ–º assets –∏–∑ HTML
        assets = self._extract_assets(content_html)

        return Post(
            post_id=post_id,
            title=title,
            content_html=content_html,
            post_date=post_date,
            source_url=post_url,
            tags=tags,
            assets=assets,
        )

    def _extract_assets(self, html_content: str) -> list[dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ HTML."""
        if not html_content:
            return []

        assets = []
        soup = BeautifulSoup(html_content, 'lxml')

        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if not src:
                continue

            # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π URL
            if not src.startswith('http'):
                src = urljoin('https://sponsr.ru', src)

            # Alt —Ç–µ–∫—Å—Ç
            alt = img.get('alt', '')
            if not alt:
                parent = img.find_parent('div', class_='post-image')
                if parent and parent.get('data-alt'):
                    alt = parent.get('data-alt')

            assets.append({'url': src, 'alt': alt})

        return assets

    def _parse_video_url(self, embed_src: str) -> str | None:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç embed URL –≤ watch URL."""
        for pattern, converter in VIDEO_EMBED_PATTERNS:
            match = re.search(pattern, embed_src)
            if match:
                return converter(match)
        # Fallback: –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL –µ—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω
        if embed_src and ('video' in embed_src or 'embed' in embed_src):
            return embed_src
        return None

    def _replace_video_embeds(self, html_content: str) -> str:
        """–ó–∞–º–µ–Ω—è–µ—Ç iframe/embed –≤–∏–¥–µ–æ –Ω–∞ markdown-—Å—Å—ã–ª–∫–∏."""
        soup = BeautifulSoup(html_content, 'lxml')

        for iframe in soup.find_all(['iframe', 'embed']):
            src = iframe.get('src', '')
            video_url = self._parse_video_url(src)
            if video_url:
                placeholder = soup.new_tag('p')
                placeholder.string = f'üìπ –í–∏–¥–µ–æ: {video_url}'
                iframe.replace_with(placeholder)

        return str(soup)

    def _to_markdown(self, post: Post, asset_map: dict[str, str]) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown."""
        if not post.content_html:
            return f"# {post.title}\n\n"

        # –ó–∞–º–µ–Ω—è–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ
        html = post.content_html
        for original_url, local_filename in asset_map.items():
            html = html.replace(original_url, f"assets/{local_filename}")

        # –ó–∞–º–µ–Ω—è–µ–º iframe/embed –≤–∏–¥–µ–æ –Ω–∞ markdown-—Å—Å—ã–ª–∫–∏
        html = self._replace_video_embeds(html)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ Markdown
        h2t = html2text.HTML2Text()
        h2t.ignore_links = False
        h2t.ignore_images = False
        h2t.body_width = 0  # –ë–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫
        h2t.unicode_snob = True

        markdown = h2t.handle(html)

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        return f"# {post.title}\n\n{markdown}"
